# End-to-End Trainable MCTS with Episodic Control

In recent years, a lot of work has been done to try to augment classical approaches taken to reinforcement learning and deep learning. In the Memory-Augmented Monte Carlo Tree Search paper (M-MCTS), Xiao et al. (2018) added a memory data structure to vanilla Monte Carlo Tree Search, showing that this could minimize error in theory and practice. We propose to investigate alternative workarounds to the shortcomings involved in the design of the memory architecture. We suggest two new methods for incorporating a differentiable memory that can we optimized end-to-end in tandem with MCTS. Our approach draws on fundamental ideas from episodic controlâ€“specifically the use of differentiable neural dictionaries (DNDs) and ephemeral value adjustments (EVA). In theory, by using an optimizable feature representation to smartly query a memory that also gets optimized with each search iteration, our new method should be able to perform better than the original M-MCTS.
